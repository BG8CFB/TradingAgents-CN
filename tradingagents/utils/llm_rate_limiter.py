"""
LLM è°ƒç”¨é€Ÿç‡é™åˆ¶å™¨

ğŸ”¥ ä¿®å¤ S11: æ·»åŠ  LLM è°ƒç”¨é€Ÿç‡é™åˆ¶ï¼Œé˜²æ­¢æˆæœ¬å¤±æ§å’Œ API é™æµ

åŠŸèƒ½ï¼š
1. åŸºäºä»¤ç‰Œæ¡¶ç®—æ³•çš„é€Ÿç‡é™åˆ¶
2. æ”¯æŒæŒ‰æä¾›å•†é…ç½®ä¸åŒçš„é™åˆ¶
3. æ”¯æŒå¹¶å‘æ§åˆ¶
4. æä¾›è°ƒç”¨ç»Ÿè®¡å’Œæˆæœ¬ä¼°ç®—
"""

import asyncio
import logging
import threading
import time
from dataclasses import dataclass, field
from typing import Dict, Optional, Any
from functools import wraps

logger = logging.getLogger("tradingagents.utils.llm_rate_limiter")


@dataclass
class RateLimitConfig:
    """é€Ÿç‡é™åˆ¶é…ç½®"""
    # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
    requests_per_minute: int = 60
    # æ¯åˆ†é’Ÿæœ€å¤§ token æ•°ï¼ˆä¼°ç®—ï¼‰
    tokens_per_minute: int = 100000
    # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    max_concurrent: int = 5
    # è¯·æ±‚é—´æœ€å°é—´éš”ï¼ˆç§’ï¼‰
    min_interval: float = 0.1
    # æ˜¯å¦å¯ç”¨
    enabled: bool = True


@dataclass
class CallStats:
    """è°ƒç”¨ç»Ÿè®¡"""
    total_calls: int = 0
    total_tokens_estimated: int = 0
    total_cost_estimated: float = 0.0
    failed_calls: int = 0
    rate_limited_calls: int = 0
    last_call_time: float = 0.0
    calls_this_minute: int = 0
    minute_start_time: float = field(default_factory=time.time)


# é»˜è®¤çš„æä¾›å•†é€Ÿç‡é™åˆ¶é…ç½®
DEFAULT_RATE_LIMITS: Dict[str, RateLimitConfig] = {
    "openai": RateLimitConfig(requests_per_minute=60, tokens_per_minute=90000, max_concurrent=5),
    "deepseek": RateLimitConfig(requests_per_minute=60, tokens_per_minute=100000, max_concurrent=5),
    "dashscope": RateLimitConfig(requests_per_minute=100, tokens_per_minute=150000, max_concurrent=10),
    "google": RateLimitConfig(requests_per_minute=60, tokens_per_minute=100000, max_concurrent=5),
    "anthropic": RateLimitConfig(requests_per_minute=50, tokens_per_minute=80000, max_concurrent=3),
    "default": RateLimitConfig(requests_per_minute=30, tokens_per_minute=50000, max_concurrent=3),
}

# ä¼°ç®—çš„ token æˆæœ¬ï¼ˆæ¯ 1000 tokensï¼Œç¾å…ƒï¼‰
TOKEN_COSTS: Dict[str, Dict[str, float]] = {
    "openai": {"input": 0.0015, "output": 0.002},
    "deepseek": {"input": 0.0001, "output": 0.0002},
    "dashscope": {"input": 0.0008, "output": 0.002},
    "google": {"input": 0.00025, "output": 0.0005},
    "anthropic": {"input": 0.003, "output": 0.015},
    "default": {"input": 0.001, "output": 0.002},
}


class LLMRateLimiter:
    """
    LLM è°ƒç”¨é€Ÿç‡é™åˆ¶å™¨
    
    ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°é€Ÿç‡é™åˆ¶ï¼Œæ”¯æŒï¼š
    - æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶
    - å¹¶å‘è¯·æ±‚æ•°é™åˆ¶
    - è¯·æ±‚é—´éš”æ§åˆ¶
    """
    
    _instance: Optional["LLMRateLimiter"] = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self._initialized = True
        self._configs: Dict[str, RateLimitConfig] = DEFAULT_RATE_LIMITS.copy()
        self._stats: Dict[str, CallStats] = {}
        self._semaphores: Dict[str, asyncio.Semaphore] = {}
        self._sync_semaphores: Dict[str, threading.Semaphore] = {}
        self._call_lock = threading.Lock()
        
        logger.info("[LLMé€Ÿç‡é™åˆ¶] åˆå§‹åŒ–å®Œæˆ")
    
    def get_config(self, provider: str) -> RateLimitConfig:
        """è·å–æä¾›å•†çš„é€Ÿç‡é™åˆ¶é…ç½®"""
        return self._configs.get(provider.lower(), self._configs["default"])
    
    def set_config(self, provider: str, config: RateLimitConfig):
        """è®¾ç½®æä¾›å•†çš„é€Ÿç‡é™åˆ¶é…ç½®"""
        self._configs[provider.lower()] = config
        logger.info(f"[LLMé€Ÿç‡é™åˆ¶] æ›´æ–° {provider} é…ç½®: {config}")
    
    def get_stats(self, provider: str) -> CallStats:
        """è·å–æä¾›å•†çš„è°ƒç”¨ç»Ÿè®¡"""
        if provider.lower() not in self._stats:
            self._stats[provider.lower()] = CallStats()
        return self._stats[provider.lower()]
    
    def _get_semaphore(self, provider: str) -> threading.Semaphore:
        """è·å–åŒæ­¥ä¿¡å·é‡"""
        provider = provider.lower()
        if provider not in self._sync_semaphores:
            config = self.get_config(provider)
            self._sync_semaphores[provider] = threading.Semaphore(config.max_concurrent)
        return self._sync_semaphores[provider]
    
    def _check_rate_limit(self, provider: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶
        
        Returns:
            True å¦‚æœå¯ä»¥ç»§ç»­ï¼ŒFalse å¦‚æœéœ€è¦ç­‰å¾…
        """
        config = self.get_config(provider)
        if not config.enabled:
            return True
        
        stats = self.get_stats(provider)
        current_time = time.time()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®åˆ†é’Ÿè®¡æ•°å™¨
        if current_time - stats.minute_start_time >= 60:
            stats.calls_this_minute = 0
            stats.minute_start_time = current_time
        
        # æ£€æŸ¥æ¯åˆ†é’Ÿè¯·æ±‚æ•°
        if stats.calls_this_minute >= config.requests_per_minute:
            return False
        
        # æ£€æŸ¥æœ€å°é—´éš”
        if current_time - stats.last_call_time < config.min_interval:
            return False
        
        return True
    
    def _wait_for_rate_limit(self, provider: str, timeout: float = 60.0) -> bool:
        """
        ç­‰å¾…ç›´åˆ°å¯ä»¥å‘èµ·è¯·æ±‚
        
        Returns:
            True å¦‚æœæˆåŠŸè·å–é…é¢ï¼ŒFalse å¦‚æœè¶…æ—¶
        """
        config = self.get_config(provider)
        if not config.enabled:
            return True
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            if self._check_rate_limit(provider):
                return True
            time.sleep(0.1)
        
        return False
    
    def _record_call(self, provider: str, tokens_estimated: int = 0, success: bool = True):
        """è®°å½•ä¸€æ¬¡è°ƒç”¨"""
        with self._call_lock:
            stats = self.get_stats(provider)
            stats.total_calls += 1
            stats.calls_this_minute += 1
            stats.last_call_time = time.time()
            stats.total_tokens_estimated += tokens_estimated
            
            if not success:
                stats.failed_calls += 1
            
            # ä¼°ç®—æˆæœ¬
            costs = TOKEN_COSTS.get(provider.lower(), TOKEN_COSTS["default"])
            # å‡è®¾è¾“å…¥è¾“å‡ºå„å ä¸€åŠ
            cost = (tokens_estimated / 1000) * (costs["input"] + costs["output"]) / 2
            stats.total_cost_estimated += cost
    
    def rate_limited_call(self, provider: str, func, *args, **kwargs):
        """
        æ‰§è¡Œé€Ÿç‡é™åˆ¶çš„åŒæ­¥è°ƒç”¨
        
        Args:
            provider: LLM æä¾›å•†åç§°
            func: è¦è°ƒç”¨çš„å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°
        
        Returns:
            å‡½æ•°è¿”å›å€¼
        """
        config = self.get_config(provider)
        
        if not config.enabled:
            return func(*args, **kwargs)
        
        # è·å–å¹¶å‘ä¿¡å·é‡
        semaphore = self._get_semaphore(provider)
        
        # ç­‰å¾…é€Ÿç‡é™åˆ¶
        if not self._wait_for_rate_limit(provider):
            stats = self.get_stats(provider)
            stats.rate_limited_calls += 1
            logger.warning(f"[LLMé€Ÿç‡é™åˆ¶] {provider} é€Ÿç‡é™åˆ¶è¶…æ—¶")
            raise RuntimeError(f"LLM rate limit exceeded for {provider}")
        
        # æ‰§è¡Œè°ƒç”¨
        acquired = semaphore.acquire(timeout=30)
        if not acquired:
            raise RuntimeError(f"LLM concurrent limit exceeded for {provider}")
        
        try:
            result = func(*args, **kwargs)
            
            # ä¼°ç®— token æ•°ï¼ˆç®€å•ä¼°ç®—ï¼šæ¯ä¸ªå­—ç¬¦çº¦ 0.5 tokenï¼‰
            tokens_estimated = 0
            if hasattr(result, 'content') and result.content:
                tokens_estimated = len(result.content) // 2
            
            self._record_call(provider, tokens_estimated, success=True)
            return result
        except Exception as e:
            self._record_call(provider, 0, success=False)
            raise
        finally:
            semaphore.release()
    
    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰æä¾›å•†çš„ç»Ÿè®¡ä¿¡æ¯"""
        result = {}
        for provider, stats in self._stats.items():
            result[provider] = {
                "total_calls": stats.total_calls,
                "total_tokens_estimated": stats.total_tokens_estimated,
                "total_cost_estimated": round(stats.total_cost_estimated, 4),
                "failed_calls": stats.failed_calls,
                "rate_limited_calls": stats.rate_limited_calls,
                "calls_this_minute": stats.calls_this_minute,
            }
        return result
    
    def reset_stats(self, provider: Optional[str] = None):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        if provider:
            if provider.lower() in self._stats:
                self._stats[provider.lower()] = CallStats()
        else:
            self._stats.clear()
        logger.info(f"[LLMé€Ÿç‡é™åˆ¶] ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®: {provider or 'å…¨éƒ¨'}")


# å…¨å±€å®ä¾‹
_rate_limiter: Optional[LLMRateLimiter] = None


def get_rate_limiter() -> LLMRateLimiter:
    """è·å–å…¨å±€é€Ÿç‡é™åˆ¶å™¨å®ä¾‹"""
    global _rate_limiter
    if _rate_limiter is None:
        _rate_limiter = LLMRateLimiter()
    return _rate_limiter


def rate_limited(provider: str):
    """
    è£…é¥°å™¨ï¼šä¸ºå‡½æ•°æ·»åŠ é€Ÿç‡é™åˆ¶
    
    ç”¨æ³•ï¼š
        @rate_limited("openai")
        def call_openai(messages):
            return llm.invoke(messages)
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            limiter = get_rate_limiter()
            return limiter.rate_limited_call(provider, func, *args, **kwargs)
        return wrapper
    return decorator
